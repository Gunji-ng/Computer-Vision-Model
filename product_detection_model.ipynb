{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1626545496614,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "wLAvf69L7Ixy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1626545496631,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "f9fDfCwVPJJJ",
    "outputId": "d276cfb7-f96d-4a97-a061-a6ffb4f97250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "INPUT_SIZE = 224\n",
    "\n",
    "image_gen_train = ImageDataGenerator(rescale=1./255, \n",
    "                                     rotation_range=15, \n",
    "                                     width_shift_range=.15, \n",
    "                                     height_shift_range=.15, \n",
    "                                     horizontal_flip=True, \n",
    "                                     vertical_flip=True,\n",
    "                                     zoom_range=0.2\n",
    "                                    )\n",
    "\n",
    "\n",
    "train_data_gen = image_gen_train.flow_from_directory(directory='./products_images/train',\n",
    "                                                     class_mode='categorical', \n",
    "                                                     target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1626545496633,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "xuJF3Ek5ctoF",
    "outputId": "2126ab80-4526-4bd3-f74f-4a2f3aa4c2b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iphone12_pro_max': 0,\n",
       " 'jbl_charge3': 1,\n",
       " 'nintendo_switch': 2,\n",
       " 'ps4_controller': 3,\n",
       " 'yeezy_boost_350': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1626545496634,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "E-rI1S6kb28D",
    "outputId": "506bb419-e0bf-4c75-84c2-1f5a44823bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 113 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_data_gen = image_gen_val.flow_from_directory(directory='./products_images/validate', \n",
    "                                                   class_mode='categorical', \n",
    "                                                   target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1626545496636,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "Hq6R1NiylAKF",
    "outputId": "e6f56842-f9c0-4f98-80df-ec1b731a5c7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iphone12_pro_max': 0,\n",
       " 'jbl_charge3': 1,\n",
       " 'nintendo_switch': 2,\n",
       " 'ps4_controller': 3,\n",
       " 'yeezy_boost_350': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5281,
     "status": "ok",
     "timestamp": 1626545501889,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "vAMwUw39cbpM"
   },
   "outputs": [],
   "source": [
    "transfer_learning_layer = tf.keras.applications.MobileNetV2(include_top=False, \n",
    "                                                      weights='imagenet', \n",
    "                                                      input_shape=(INPUT_SIZE, INPUT_SIZE, 3)\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1626545501919,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "l3fsVSLecmWo"
   },
   "outputs": [],
   "source": [
    "transfer_learning_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1626545503292,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "50P-oWzGcpR2"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    transfer_learning_layer,\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1626545503299,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "dxCaXwIgczP1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0), \n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77257,
     "status": "ok",
     "timestamp": 1626545580547,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "27IFAI4Ijdxs",
    "outputId": "c8da60c0-a271-4ce0-95c2-d107f232b094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "310/310 [==============================] - 12s 28ms/step - loss: 5.9016 - accuracy: 0.6871 - val_loss: 3.1120 - val_accuracy: 0.8850\n",
      "Epoch 2/4\n",
      "310/310 [==============================] - 8s 24ms/step - loss: 2.9443 - accuracy: 0.8710 - val_loss: 1.6567 - val_accuracy: 0.9469\n",
      "Epoch 3/4\n",
      "310/310 [==============================] - 8s 25ms/step - loss: 2.5437 - accuracy: 0.9097 - val_loss: 0.9561 - val_accuracy: 0.9381\n",
      "Epoch 4/4\n",
      "310/310 [==============================] - 8s 25ms/step - loss: 3.1357 - accuracy: 0.9323 - val_loss: 2.0985 - val_accuracy: 0.9292\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "history = model.fit(train_data_gen, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=val_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95335,
     "status": "ok",
     "timestamp": 1626546286465,
     "user": {
      "displayName": "Damilola Egunjobi",
      "photoUrl": "",
      "userId": "04788411531026237659"
     },
     "user_tz": -60
    },
    "id": "GRp2mS2kjgxZ",
    "outputId": "1ac73779-f13f-410a-eaea-c95ea3a906e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n",
      "C:\\Users\\Gunji\\Anaconda3\\envs\\data_science_projects\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\extracted_product_detection_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\extracted_product_detection_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('.\\extracted_product_detection_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "ps4_controller_test_data_gen = image_gen_test.flow_from_directory(directory='./products_images/test', \n",
    "                                                   class_mode='categorical', \n",
    "                                                   target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-168.09169  , -106.938934 ,  -47.878033 ,  106.91432  ,\n",
       "         -51.441925 ],\n",
       "       [-158.89769  ,   31.668007 ,  -40.66683  ,  -80.12598  ,\n",
       "         -75.5634   ],\n",
       "       [-149.68968  , -167.94661  ,  -93.22367  ,   76.06532  ,\n",
       "         -51.29579  ],\n",
       "       [-214.37602  , -148.7279   ,   -0.6977628,   94.38679  ,\n",
       "         -79.140785 ],\n",
       "       [-204.69273  , -116.82195  ,  -35.10863  ,   65.794655 ,\n",
       "         -66.88236  ],\n",
       "       [-262.6041   , -184.733    ,  -73.09801  ,  133.00174  ,\n",
       "         -51.110992 ],\n",
       "       [-143.66011  ,  -96.94547  ,  -20.386663 ,   82.67798  ,\n",
       "         -75.36298  ],\n",
       "       [-221.4947   ,  171.18665  ,  -43.155212 , -116.659096 ,\n",
       "         -61.94471  ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(ps4_controller_test_data_gen)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_bool = (test_pred > 1)\n",
    "test_predictions = test_pred_bool.astype(int)\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('test_mug.jpg')\n",
    "\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = tf.concat(image, axis=0)\n",
    "image = tf.image.resize(image, [INPUT_SIZE, INPUT_SIZE])\n",
    "image = tf.cast(image, tf.float32)\n",
    "image /= 255\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-109.40728 ,  -48.67871 ,   -9.58495 ,   -8.810301,  -93.785065]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(image)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bool = (prediction > 1)\n",
    "pred_bool = pred_bool.astype(int)\n",
    "pred_bool"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "product_detection_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
